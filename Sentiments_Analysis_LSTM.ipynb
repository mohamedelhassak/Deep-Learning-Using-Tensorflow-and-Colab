{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiments_Analysis_LSTM.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMjehOSoL2hr25lYqTYUAw+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohamedelhassak/Deep-Learning-Using-Tensorflow-and-Colab-Training/blob/master/Sentiments_Analysis_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gztyVjZ300UH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "e9bd5102-96c9-4730-acde-499cfde3b20b"
      },
      "source": [
        "# import section \n",
        "#Import Libraries\n",
        "import numpy\n",
        "from numpy import array\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM, Dropout\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import load_model\n",
        "import re\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ptNSGpl487X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "bae975ba-7748-456a-b3e0-1167d786549e"
      },
      "source": [
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)\n",
        "\n",
        "# Load the dataset but only keep the top n words and zero out the rest i.e keep vocabulary size as 5000\n",
        "top_words = 5000 #vocabulary_size = 5000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOnUtr6F4uA8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "cd9b65d9-bc87-42d9-e1de-1b2ed1a563b7"
      },
      "source": [
        "'''Inspect a sample review and its label.Note that the review is stored as a sequence of integers. These are word IDs that \n",
        "have been pre-assigned to individual words, and the label is an integer (0 for negative, 1 for positive).'''\n",
        "print('---review---')\n",
        "print(X_train[6])\n",
        "print('---label---')\n",
        "print(y_train[6])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---review---\n",
            "[1, 2, 365, 1234, 5, 1156, 354, 11, 14, 2, 2, 7, 1016, 2, 2, 356, 44, 4, 1349, 500, 746, 5, 200, 4, 4132, 11, 2, 2, 1117, 1831, 2, 5, 4831, 26, 6, 2, 4183, 17, 369, 37, 215, 1345, 143, 2, 5, 1838, 8, 1974, 15, 36, 119, 257, 85, 52, 486, 9, 6, 2, 2, 63, 271, 6, 196, 96, 949, 4121, 4, 2, 7, 4, 2212, 2436, 819, 63, 47, 77, 2, 180, 6, 227, 11, 94, 2494, 2, 13, 423, 4, 168, 7, 4, 22, 5, 89, 665, 71, 270, 56, 5, 13, 197, 12, 161, 2, 99, 76, 23, 2, 7, 419, 665, 40, 91, 85, 108, 7, 4, 2084, 5, 4773, 81, 55, 52, 1901]\n",
            "---label---\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXfQWUtk5DEl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "0484e2d3-0198-486f-8387-5599e0f5d118"
      },
      "source": [
        "'''We can use the dictionary returned by imdb.get_word_index() to map the review back to the original words.'''\n",
        "word2id = imdb.get_word_index()\n",
        "id2word = {i: word for word, i in word2id.items()}\n",
        "print('---review with words---')\n",
        "print([id2word.get(i, ' ') for i in X_train[6]])\n",
        "print('---label---')\n",
        "print(y_train[6])\n",
        "\n",
        "print(word2id)\n",
        "\n",
        "print(id2word)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 1s 1us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ9-R7bZ5KAw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "e988231d-5323-464c-e0d9-8aaa38d80011"
      },
      "source": [
        "#Maximum review length and minimum review length.\n",
        "print('Maximum review length: {}'.format(\n",
        "len(max((X_train + X_test), key=len))))\n",
        "\n",
        "print('Minimum review length: {}'.format(\n",
        "len(min((X_train + X_test), key=len))))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum review length: 2697\n",
            "Minimum review length: 70\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CSYnBbn5Ob8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''In order to feed this data into our RNN, all input documents must have the same length. We will limit the maximum review length to maximum words by truncating \n",
        "longer reviews and padding shorter reviews with a null value (0). We can accomplish this task using the pad_sequences() function in Keras. Here, setting max_review_length \n",
        "to 500.'''\n",
        "\n",
        "max_review_length = 500\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNl5tLRc5RIk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Remember that our input is a sequence of words (technically, integer word IDs) of maximum length = max_review_length, and our output is a binary sentiment \n",
        "label (0 or 1).\n",
        "'''\n",
        "# create the model\n",
        "embedding_vecor_length = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIvTmA3X5Uqq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525
        },
        "outputId": "b41159fd-57cd-4f34-f429-8f45a0d74d4b"
      },
      "source": [
        "'''We first need to compile our model by specifying the loss function and optimizer we want to use while training, as well as any evaluation metrics \n",
        "we’d like to measure. Specify the appropriate parameters, including at least one metric ‘accuracy’.'''\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=30, batch_size=64)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 500, 32)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 213,301\n",
            "Trainable params: 213,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/30\n",
            "25000/25000 [==============================] - 312s 12ms/step - loss: 0.4496 - accuracy: 0.7820 - val_loss: 0.4937 - val_accuracy: 0.7744\n",
            "Epoch 2/30\n",
            "25000/25000 [==============================] - 313s 13ms/step - loss: 0.3119 - accuracy: 0.8704 - val_loss: 0.3039 - val_accuracy: 0.8715\n",
            "Epoch 3/30\n",
            "25000/25000 [==============================] - 310s 12ms/step - loss: 0.2477 - accuracy: 0.9028 - val_loss: 0.3419 - val_accuracy: 0.8530\n",
            "Epoch 4/30\n",
            " 1472/25000 [>.............................] - ETA: 3:54 - loss: 0.2138 - accuracy: 0.9212"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uj_3f9Ad5ZiS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Calculate Accuracy\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E95uN4Bg5c-1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Run these codes first in order to install the necessary libraries and perform authorization\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "#Mount your Google Drive:\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCJgCS5l5hGE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#After success run Drive FUSE program, you can create a directory Sentiment_Analysis and access your drive at /content/drive with using command\n",
        "import os\n",
        "os.mkdir(\"/content/drive/Sentiment_Analysis\")\n",
        "os.chdir(\"/content/drive/\")\n",
        "!ls\n",
        "\n",
        "#Append your path\n",
        "import sys\n",
        "sys.path.append('/content/drive/Sentiment_Analysis')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBb6Ft9c5kRb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Now save the model in required directory\n",
        "model.save('/content/drive/Sentiment_Analysis/sentiment_analysis_model_new.h5')\n",
        "print(\"Saved model to disk\")\n",
        "\n",
        "#Check the content of the directory\n",
        "os.chdir(\"/content/drive/Sentiment_Analysis\")\n",
        "!ls\n",
        "\n",
        "#Code to load the saved model\n",
        "model = load_model('/content/drive/Sentiment_Analysis/sentiment_analysis_model_new.h5')\n",
        "print(\"Model Loaded\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}